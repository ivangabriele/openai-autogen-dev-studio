{
  // Brave Search API Key
  // Used by agent function `search_web()` which agents can use to help with their task
  // https://brave.com/search/api/ (there is a free plan)
  // "Data for AI" plan is prefered over "Data for Search" since it includes some useful additional props
  "brave_search_api_key": "[BRAVE_SEARCH_API_KEY]",

  // Selected `model` that agents must use in the list of OpenAI API endpoints
  "current_model": "Open-Orca/Mistral-7B-OpenOrca",

  // Description of the program you want the agents to develop
  // You can also set it to `null` if you want the Product Owner agent to prompt you each time you run OADS.
  "initial_project_description": "Quickly develop a basic CLI snake game in Python.",

  // List of OpenAI API endpoints
  // The `model` key must be unique.
  // https://microsoft.github.io/FLAML/docs/reference/autogen/oai/completion/#create
  "models": [
    // Any model using Azure OpenAI API
    {
      "model": "[AZURE_OPENAI_STUDIO_DEPLOYMENT_NAME]",
      "api_base": "https://[AZURE_OPENAI_RESOURCE_NAME].openai.azure.com",
      "api_key": "[AZURE_OPENAI_API_KEY]",
      "api_type": "azure",
      // https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions
      "api_version": "2023-08-01-preview"
    },

    // `gpt-3.5-turbo-16k` using OpenAI API
    {
      "model": "gpt-3.5-turbo-16k",
      "api_key": "[OPEN_AI_API_KEY]"
    },

    // `gpt-4` using OpenAI API
    {
      "model": "gpt-4",
      "api_key": "[OPEN_AI_API_KEY]"
    },

    // `gpt-4-32k` using OpenAI API
    {
      "model": "gpt-4-32k",
      "api_key": "[OPEN_AI_API_KEY]"
    },

    // Custom LLM deployment (i.e.:: `Open-Orca/Mistral-7B-OpenOrca`)
    // using "Text generation web UI" with `OpenAI` extension enabled:
    // https://github.com/oobabooga/text-generation-webui/tree/main/extensions/openai#an-openedai-api-openai-like
    // FUNCTIONS ARE NOT SUPPORTED BY THIS "Text generation web UI" EXTENSION (YET?):
    // The workaround is to set the `user_proxy_agent.current_model` in this config to `gpt-3.5-turbo-16k`.
    // It shouldn't cost too much since the User Proxy Agent will only be used for general short replies and functions.

    // This can be also be any inference endpoint compatible following OpenAI API specs,
    // regardless of the model you use behind it.
    {
      "model": "Open-Orca/Mistral-7B-OpenOrca",
      "api_base": "https://localhost:5001/v1",
      "api_key": "[CUSTOM_API_KEY]",
      "api_type": "open_ai"
    }
  ],

  // User Proxy Agent Custom Configuration
  // This overrides the above config for the `autogen.UserProxyAgent`,
  // which is particularly useful to allow functions if you use a custom endpoint that doesn't support them.
  "user_proxy_agent": {
    // If you just want to use the same model as the other agents, you can just set this to `null`.
    "current_model": "gpt-3.5-turbo-16k"
  }

  // TODO Either remove this commented part or make it work. Decision will depend on how fast the related PR is merged.
  // Experimental features:
  // - Non-OpenAI API models, using LiteLLM
  //   This is a workaround unitil the pull request https://github.com/microsoft/autogen/pull/95 is merged.
  // "experimental_features": {
  //   "is_enabled": false,

  //   "litellm_models": [
  //     // `Open-Orca/Mistral-7B-OpenOrca` using oobabooga Text generation web UI API extension
  //     {
  //       "model": "oobabooga/Mistral-7B-OpenOrca",
  //       "api_base": "[OPEN_AI_API_KEY]"
  //     }
  //   ]
  // }
}
